分布式系统课程: [6.5840 Schedule: Spring 2024 (mit.edu)](https://pdos.csail.mit.edu/6.824/schedule.html)

why distrubute: 

*   connect physically separeted machines: 本质上为了实现 sharing
*   increase capacity through parallelism
*   fault tolerance: 服务可用性 (availability)
*   achieve security: 将安全相关的服务进行拆分, 所有其他服务通过接口的方式访问安全服务 (via isolation)

challenge:

*   many concurrent parts: 目前只做了一个 map reduce, 确实, 并发问题需要重视, 最开始的一版代码在 crash test 中出现了死锁问题
*   must deal with partial failure: map reduce 的死锁问题就是部分 client crash 导致的 ... 做不好系统容错的话, 那么并发数量一上来大概率出现并发问题
*   tricky to realize performance benfit: 最开始可能是从性能角度出发做的分布式, 但实际的性能表现可能还不如中心化的应用 ...

整个课程的 lab 主要实现了一个基于 raft 的分布式服务, 主要关注分布式服务的几个方面:

*   fault tolerance: 所谓容错, 还可以分为 availability (在 raft 中通过 replication 实现) 与 recoverability (在 raft 中通过 logging 实现, 本质上通过 durable storage 实现, 通过写磁盘的方式持久化更改)

*   consistency: 服务是分布式部署的, 自然需要让分布式设备具备一致性, 在 raft 中实现的是最终一致性

*   performance: 很多情况下 performance 与上面两点是冲突的: 如果希望实现强一致性, 那么 leader 需要写入所有的 fellower 之后才能返回, 性能折损; performance 可以从 throughput 与 latency 两方面考虑, 其中 low latency 会更难实现 ...

    >   在 map reduce 中提到了, tail latency 的情况, 即一个集群中某个设备出现了高时延, 可能会导致整个集群服务均表现为高时延

# Thread/RPC in go

整个课程是基于 go 实现的, 其中用的最多的就是 go routine 与 go rpc, 这节其实是语法课

在 go 中通过关键字 **go** 创建一个 go routine, 一般认为一个 go routine 是一个协程 (轻量级线程), 多个协程之间是并发的; go 协程的任务可以通过函数的方式进行定义, 在分布式课程中经常能看到在一个 main 函数中, 定义另外一个任务函数的写法

IO cocurrency: 对于分布式系统而言, 每次 RPC 调用都对应了网络 IO, 不管是 client 还是 server, 发出 RPC 请求, 等待 RPC 响应的过程是阻塞的, 通过多协程的调度, 当协程发生 IO 阻塞时切换其他协程执行, 提高程序执行效率

multicore parallelism: 实际中的设备都是采用多个核心设计的, 所以 go routine 之间其实是并行调度的

>   除此之外, 采用 go routine 可以很容易实现周期性任务调度, 将周期任务安排在一个 go routine 中, 通过循环与 sleep 的方式执行周期任务 (在 map reduce 中通过周期任务检查某些 RPC 是否超时)

并发环境下肯定是需要考虑竞争问题的, 在 go 中一般通过两种方式解决竞争问题:

*   mutex: 传统派 (lock + condition)
*   channel: channel 其实避免了 routine 之间对共享变量的读写, routine 之间通过 channel 进行通信

mit 分别使用 mutex 与 channel 实现了一个 crawler, 为了起到说明作用, 首先实现了一个单协程的 crawler

```go
//
// Fetcher
//
type Fetcher interface {
	// Fetch returns a slice of URLs found on the page.
	Fetch(url string) (urls []string, err error)
}

// fakeFetcher is Fetcher that returns canned results.
type fakeFetcher map[string]*fakeResult

type fakeResult struct {
	body string
	urls []string
}

// fakeFatcher 实现了方法 Fatch, 是一个 Fatcher 的实例
func (f fakeFetcher) Fetch(url string) ([]string, error) {
	if res, ok := f[url]; ok {
		fmt.Printf("found:   %s\n", url)
		return res.urls, nil
	}
	fmt.Printf("missing: %s\n", url)
	return nil, fmt.Errorf("not found: %s", url)
}

// fetcher is a populated fakeFetcher.
// fakeFatcher 本质上是一个 map, 保存了数据, 不需要真的进行请求
var fetcher = fakeFetcher{
	"http://golang.org/": &fakeResult{
		"The Go Programming Language",
		[]string{
			"http://golang.org/pkg/",
			"http://golang.org/cmd/",
		},
	},
	"http://golang.org/pkg/": &fakeResult{
		"Packages",
		[]string{
			"http://golang.org/",
			"http://golang.org/cmd/",
			"http://golang.org/pkg/fmt/",
			"http://golang.org/pkg/os/",
		},
	},
	"http://golang.org/pkg/fmt/": &fakeResult{
		"Package fmt",
		[]string{
			"http://golang.org/",
			"http://golang.org/pkg/",
		},
	},
	"http://golang.org/pkg/os/": &fakeResult{
		"Package os",
		[]string{
			"http://golang.org/",
			"http://golang.org/pkg/",
		},
	},
}

//
// Serial crawler, 简单来说就是通过 DFS 进行递归调用
// 采用 map 的方式标记当前 url 是否已经遍历过了, 因为是单协程的程序, 不需要关心并发安全问题
//
func Serial(url string, fetcher Fetcher, fetched map[string]bool) {
	if fetched[url] {
		return
	}
	fetched[url] = true
	urls, err := fetcher.Fetch(url)
	if err != nil {
		return
	}
	for _, u := range urls {
		Serial(u, fetcher, fetched)
	}
	return
}
```

对于真实的 crawler 而言, 方法 Fetch() 需要爬取网页数据, 可能阻塞, 为了尽可能利用 IO concurrency, 优化为每个协程爬取一个 url

```go
//
// Concurrent crawler with shared state and Mutex
//
type fetchState struct {
	mu      sync.Mutex
	fetched map[string]bool
}

func (fs *fetchState) testAndSet(url string) bool {
	fs.mu.Lock()
	defer fs.mu.Unlock()
	r := fs.fetched[url]
	fs.fetched[url] = true
	return r
}

func ConcurrentMutex(url string, fetcher Fetcher, fs *fetchState) {
	if fs.testAndSet(url) {
		return
	}
	urls, err := fetcher.Fetch(url)
	if err != nil {
		return
	}
	var done sync.WaitGroup
	for _, u := range urls {
		done.Add(1)
		go func(u string) {
			ConcurrentMutex(u, fetcher, fs)
			done.Done()
		}(u)
	}
	done.Wait()
	return
}
```

本质上 fetchState 也是一个 map, 只不过在多协程环境下, 为了保证 map 的线程安全性, 还需要通过锁保证一致性, 直接将 fetchState 看成是一个带锁的 map 即可

函数 ConcurrentMutex 也是一个递归函数, 且每次递归之前还会调用 testAndSet, 保证仅在当前 url 未被处理的情况下, 再递归处理当前 url

借助 go 的 channel 特性可以实现 lock-free 的并发控制

```go 
//
// Concurrent crawler with channels
//
func worker(url string, ch chan []string, fetcher Fetcher) {
	urls, err := fetcher.Fetch(url)
	if err != nil {
		ch <- []string{}
	} else {
		ch <- urls
	}
}

func coordinator(ch chan []string, fetcher Fetcher) {
	n := 1
	fetched := make(map[string]bool)
	for urls := range ch {
		for _, u := range urls {
			if fetched[u] == false {
				fetched[u] = true
				n += 1
				go worker(u, ch, fetcher)
			}
		}
		n -= 1
		if n == 0 {
			break
		}
	}
}

func ConcurrentChannel(url string, fetcher Fetcher) {
	ch := make(chan []string)
	go func() {
		ch <- []string{url}
	}()
	coordinator(ch, fetcher)
}
```

注意到基于 channel 实现的多协程版 crawler, 每个创建出来的协程不再检查当前 url 是否已经访问过了, worker 协程直接将当前 url 指向的各个 sub-url 填充到 channel 中

而只有 coordinator 协程负责检查当前 url 是否已经访问过了, 即 map 仅存在于 coordinator 而不是 worker 中, 因为避免了共享变量, 因此也实现了 lock-free

go 原生库函数提供了 RPC 的实现, 可以通过类似 local procedure call 的方式实现 RPC, 要注意的是, 使用结构体作为 RPC 调用的参数/返回值/方法名时, 需要首字母大写 ...

RPC semantic under failure (简单来说就是 client 发生 RPC 调用失败之后的策略): 

*   at-least-once: RPC 调用失败后, 会重复调用, 直到一次成功, 或者抵达调用上限 -> 这种策略可能会导致 server 端对同一个 RPC 执行多次 
*   at-most-once: RPC 调用在 server 端最多只会执行一次, server 需要识别重复的 RPC 调用, 保证不会重复执行 (duplication 实现)
*   exactly-once: RPC 调用在 server 端执行的次数为一次, 不多也不少, 相当于在 at-most-once 的基础上添加无限制的重试策略, 开销很大, 此外还需要通过持久化策略, 保证 server 端重启后, 也保留了更改

>   go 库函数实现了 at-most-once 的 RPC 调用

# [MapReduce](https://pdos.csail.mit.edu/6.824/labs/lab-mr.html)

建议先看一下论文: [map reduce](https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf)

map reduce 提供的是一个框架, 对于开发者而言, 只需要关心 map function 与 reduce function 的实现, 不管是 map function 还是 reduce function, 都是 sequential 的, 即开发者在填充任务时, 不需要考虑分布式情况下, 并发安全性问题, 而实际的任务的分布式调度完全交由了 map reduce 框架; 一个中心化的任务只要可以拆分为 map + reduce 两个任务就可以实现分布式调度; 这个 lab 要实现的就是 map reduce 框架

map reduce 面向海量输入的场景, 假设当前任务的输入集合是通过若干文件呈现的: map-reduce 框架首先会并发的为每个文件调用 map function, 并将结果保存在 intermediate files 中; 在所有 map function 完成之后, 会以所有的 intermediate file 为输入, 调用 reduce function, 将多个 intermediate file 的结果通过 reduce function 完成聚合, 得到最终结果, 

从执行上来看不管是 map function 还是 reduce function, 其输入集合都是独立的, 两种任务都可以并行执行, 效率很高, map-reduce 的主要开销是: 任务整体从 map 阶段到 reduce 阶段的转换开销, reduce 需要等待所有 map 完成之后才能进行

对于这个 lab 的 coordinator 而言, 先 map 再 reduce 的调度并不难实现, 难点在于如何实现 fault tolerance: rerun map/reduce task

worker 与 coordinator 之间是通过 RPC 调用通信的, 因此 coordinator 无法直接获取 worker 当前执行的状态, 再当前 lab 中, coordinator 会假设, 如果 worker 在一定时间内没有给予 coordinator 响应, 那么当前任务执行失败, 需要重新调度

在 map-reduce 中不管是 map function 还是 reduce function 都会因为上述容错的机制, 出现相同函数多次执行的情况, 因此 map-reduce 主要面向的场景是无状态的场景, 保证在相同输入的情况下多次执行 map/reduce task 的输出是不变的; 也正是因为 map/reduce function 可以执行多次的特性, 使得 map-reduce 中不太可能出现 tail latency 的情况: 就算某个 task 被分配给了 slow worker, coordinator 还可以通过将一个任务分配给多个 worker 的方式避免被 slow worker 拖慢

在 map/reduce lab 中需要修改 coordinator.go, worker.go, rpc.go 实现一个简易的 map-reduce 框架

map-reduce 的实现重点在于实现 worker 与 coordinator 之间的交互:

*   worker 开机之后, 会不断向 coordinator 发出 RPC 请求任务
*   coordinator 需要会维护当前任务集合, 如果 map 任务尚未完成, 则先分配 map 任务, 否则分配 reduce 任务; 要注意的是 coordinator 还需要考虑容错, coordinator 需要记录每个任务的分配时间, 通过定期扫描的方式, 重新分配超时的任务, 在 guide book 中, 提到可以将超时时间设置为 10s, 即任务分配出去后, 如果 10s 后还未收到响应则认为任务失败

input files 在经过了 map function 处理后得到了 intermediate files, reduce function 的输入为 intermediate files; 这里先对 intermediate files 的表现定义: 每个 coordinator 在开机之后需要传入 nReduce 参数, 表示 reduce task 的数目, 因此这里认为 intermediate files 的数目就是 nReduce, 由于 map function 的结果是通过键值对的方式表示的, 因此每个 input file 调用 map function 得到键值对之后, 按照 key hash 的方式映射到某个 intermediate file 中

```go
// rpc.go
type TaskType int

// 定义 worker 的任务类型
// worker 会在任务类型为 None 时认为 map reduce 已经完成了, 直接退出
const (
	None TaskType = iota
	Map
	Reduce
)

// Add your RPC definitions here.
type RPCRequset struct {
	WorkerId         int
	TaskType         TaskType
	FinishedFile     string
	IntermediateFile string
}

type RPCResponse struct {
	WorkerId  int
	TaskType  TaskType
	InputFile string
	NReduce   int
}
```

worker 会向 coordinator 发起 RPCRequest 获取 task, 而 coordinator 会使用 RPCResponse 作为响应

```go
// worker.go

// main/mrworker.go calls this function.
func Worker(mapf func(string, string) []KeyValue,
	reducef func(string, []string) string) {

	request := RPCRequset{}
	request.TaskType = None

	// initial request: get worker id
	response := InitialRequest(request)
	nReduce := response.NReduce
	// set worker id
	request.WorkerId = response.WorkerId

	for {
		// request for task
		response := RequestForTask(request)
		// no task
		if response.TaskType == None {
			break
		}

		request.TaskType = response.TaskType
		if response.TaskType == 1 {
			request.IntermediateFile = MapTask(mapf, response.InputFile, nReduce, request.WorkerId)
		} else if response.TaskType == 2 {
			intermediate, err := strconv.Atoi(response.InputFile)
			if err != nil {
				log.Fatalf("cannot convert %v", response.InputFile)
			}
			request.IntermediateFile = ReduceTask(reducef, intermediate, request.WorkerId)
		}
		request.FinishedFile = response.InputFile
	}
}

func InitialRequest(request RPCRequset) RPCResponse {
	response := RPCResponse{}

	// send the RPC request, wait for the reply.
	ok := call("Coordinator.InitialRequest", &request, &response)
	if ok {
		fmt.Printf("initial worker id %v\n", response.WorkerId)
	} else {
		fmt.Printf("call failed!\n")
	}
	return response
}

func RequestForTask(request RPCRequset) RPCResponse {
	// declare a reply structure.
	response := RPCResponse{}

	// send the RPC request, wait for the reply.
	ok := call("Coordinator.RequestForTask", &request, &response)
	if ok {
		fmt.Printf("worker %v task type %v\n", request.WorkerId, response.TaskType)
	} else {
		fmt.Printf("worker %v call failed!\n", request.WorkerId)
		// the worker assume the coordinator is down
		response.TaskType = None
	}
	return response
}
```

整个 worker 的主程序由一个 for 循环构成, worker 会不断向 coordinator 发起 RPC 请求获取下一个执行的任务; 值得注意的是这里的实现中, worker 在执行任务之前, 首先会通过 InitialRequest 向 coordinator 获取一个全局唯一的 worker id, 随后根据获取到的 task 类型: map/reduce 调用对应的函数

6.5840 的测试环境使用多个 worker 进程模拟分布式设备上的 worker 示例, 这样多个 worker 之间在 6.5840 的测试环境的文件系统是公共的, 一般而言, 如果一个 worker 执行 map task 失败了, 那么其对应的 intermediate files 是不应该被 reduce task 看到的; guide book 提到了一个 trick 专门解决这个问题: 先将结果写道 temp file 中, 随后在完成全部写入之后对 temp file 进行重命名

map-reduce 的 intermediate file 中保存的是键值对, 实验指导书建议利用 go 本身的库函数以 json 的形式保存/读取各个键值对

```go
// worker.go

func MapTask(mapf func(string, string) []KeyValue, filename string, nReduce int, workerId int) string {
	// 首先读取当前 map task 对应的 input file
    file, err := os.Open(filename)
	if err != nil {
		log.Fatalf("cannot open %v", filename)
	}
	defer file.Close()

	content, err := io.ReadAll(file)
	if err != nil {
		log.Fatalf("cannot read %v", filename)
	}
    // 调用 map function 操作 inputfile
	intermediate := mapf(filename, string(content))
	
    // 根据 reduce task 的数量创建若干 temp file 及对应的 json encoder
	tempFiles := make([]*os.File, nReduce)
	encoders := make([]*json.Encoder, nReduce)

	subDir := CreateTempDir(workerId)

	for i := 0; i < nReduce; i++ {
		tempFile, err := os.CreateTemp(subDir, "mr-intermediate-*")
		if err != nil {
			log.Fatalf("cannot open %v", err)
		}

		tempFiles[i] = tempFile
		encoders[i] = json.NewEncoder(tempFile)
	}
	
    // 根据 hash 映射的方式将各个键值对写入 temp file
	for _, kv := range intermediate {
		idx := ihash(kv.Key) % nReduce
		err := encoders[idx].Encode(&kv)
		if err != nil {
			log.Fatalf("cannot encode %v", err)
		}
	}

	hashFile := ihash(filename)
    // 为各个 intermediate file 重命名, 格式为 mr-intermediate-{worker id}-{hash(input file name)}-{reduce-id}
	filePrefix := fmt.Sprintf("mr-intermediate-%d-%d", workerId, hashFile)
	for i := 0; i < nReduce; i++ {
		// rename to final file -> the struct of intermediate file is "mr-intermediate-{map worker id}-{hash(filename)}-{reduce id}"
		err = os.Rename(tempFiles[i].Name(), fmt.Sprintf("%s-%d", filePrefix, i))
		if err != nil {
			log.Fatalf("cannot rename %v", err)
		}
		tempFiles[i].Close()
	}
	return filePrefix
}
```

值得注意的是, map function 的返回值为一个前缀, 表明了当前 map task 的 worker id 与当前执行的文件名 (hash 格式)

每个 ReduceTask 都是针对某个后缀的 intermediate file 而言的, 一个 ReduceTask 的输入来自于多个 intermediate files

```go
// worker.go

// for sorting by key.
type ByKey []KeyValue

// for sorting by key.
func (a ByKey) Len() int           { return len(a) }
func (a ByKey) Swap(i, j int)      { a[i], a[j] = a[j], a[i] }
func (a ByKey) Less(i, j int) bool { return a[i].Key < a[j].Key }

func ReduceTask(reducef func(string, []string) string, intermediate int, workerId int) string {
	// 采用通配符匹配一类 intermediate file
    ifilename := fmt.Sprintf("mr-intermediate-*-%d", intermediate)

	files, err := filepath.Glob(ifilename)
	if err != nil {
		log.Fatalf("cannot open %v", ifilename)
	}

	kvs := []KeyValue{}
	// read all intermediate files
	for _, filename := range files {
		file, err := os.Open(filename)
		if err != nil {
			log.Fatalf("cannot open %v", filename)
		}

		dec := json.NewDecoder(file)
		for {
			var kv KeyValue
			if err := dec.Decode(&kv); err != nil {
				break
			}
			kvs = append(kvs, kv)
		}
		file.Close()
	}
	// 下面的代码基本上都是从 mrsequential 中粘过来的 ...
	sort.Sort(ByKey(kvs))

	subDir := CreateTempDir(workerId)
	tempFile, err := os.CreateTemp(subDir, "mr-out-*")

	if err != nil {
		log.Fatalf("cannot open %v", err)
	}
	defer tempFile.Close()
	
	i := 0
	for i < len(kvs) {
		j := i + 1
		for j < len(kvs) && kvs[j].Key == kvs[i].Key {
			j++
		}
		values := []string{}
		for k := i; k < j; k++ {
			values = append(values, kvs[k].Value)
		}
		output := reducef(kvs[i].Key, values)

		// this is the correct format for each line of Reduce output.
		fmt.Fprintf(tempFile, "%v %v\n", kvs[i].Key, output)
		i = j
	}

	// rename to final file -> the struct of intermediate file is "mr-out-{worker id}-{reduce id}"
	filePrefix := fmt.Sprintf("mr-out-%d-%d", workerId, intermediate)
	err = os.Rename(tempFile.Name(), filePrefix)
	if err != nil {
		log.Fatalf("cannot rename %v", err)
	}

	return filePrefix
}
```

值得注意的是 reduce task 的最终产物是一个 output file, 重命名的 output file 包含了 worker id 与 reduce id 的信息

coordinator 的实现需要记录当前 task 的执行状态, 还需要记录当前 task 的执行时间, 使得超时后可以被其他 worker 重新执行

```go
// coordinator.go

type TaskState int

const (
	NotStarted TaskState = iota
	InProgress
	Completed
)

// 不仅需要记录当前 task 的执行时间, 还需要使用条件变量, 阻塞那些等待当前任务失败的 worker
type TargetFile struct {
	fileName        string
	state           TaskState
	lastExecuteTime time.Time
	lock            sync.Mutex // a lock for condition
	condition       *sync.Cond // worker may wait for
}

// 每种类型的 task 都由若干文件组成
// 通过公共标识 finished 标识当前类型的 task 是否都已经完成了
type FileBundle struct {
	files     []TargetFile // index -> file
	fileToIdx map[string]int // filename -> index
    doneCount int32 // 记录当前已完成任务的数量
	finished  bool // 实际用来标识整组 task 是否完成了
}

type Coordinator struct {
	// Your definitions here.
	nextWorkerId      int
	lock              sync.Mutex
	nReduce           int
    // inputFile 对应 map task, intermediateFiles 对应 reduce task
	inputFiles        FileBundle
	intermediateFiles FileBundle
}
```

某个 task 的超时检测由一个独立的 monitor routine 执行

```go
// coordinator.go

func (c *Coordinator) monitor() {
	for {
		time.Sleep(time.Second)
		
		if c.Done() {
			break
		}
		
        if !c.inputFiles.finished {
			for i := 0; i < len(c.inputFiles.files); i++ {
                file := &c.inputFiles.files[i]
                // file 的状态不会出现回滚, 只要被标识为 Compeleted, 标识当前文件已经被处理过了, 可以跳过
                if file.state == Completed {
                    continue
                }

                file.lock.Lock()
                if file.state == InProgress && time.Since(file.lastExecuteTime) > MAX_EXECUTE_TIME {
                    file.condition.Broadcast()
                }
                file.lock.Unlock()
            }
		}

		for i := 0; i < len(c.intermediateFiles.files); i++ {
			file := &c.intermediateFiles.files[i]
            if file.state == Completed {
			 	continue
			}

			file.lock.Lock()
			if file.state == InProgress && time.Since(file.lastExecuteTime) > MAX_EXECUTE_TIME {
				file.condition.Broadcast()
			}
			file.lock.Unlock()
		}
	}
}

// main/mrcoordinator.go calls Done() periodically to find out => unsync state
// if the entire job has finished.
func (c *Coordinator) Done() bool {
	// Your code here.
	return c.intermediateFiles.finished
}
```

在响应 worker 的请求之前 coordinator 首先需要进行若干初始化 ...

```go
// coordinator.go

// create a Coordinator.
// main/mrcoordinator.go calls this function.
// nReduce is the number of reduce tasks to use.
func MakeCoordinator(files []string, nReduce int) *Coordinator {
	// Your code here.
	c := Coordinator{}
	c.nextWorkerId = 0
	c.nReduce = nReduce
	c.inputFiles = FileBundle{files: make([]TargetFile, len(files)), fileToIdx: make(map[string]int, len(files)), doneCount: 0}
	c.intermediateFiles = FileBundle{files: make([]TargetFile, nReduce), fileToIdx: make(map[string]int, nReduce), doneCount: 0}

	for idx, file := range files {
		c.inputFiles.files[idx] = TargetFile{}
		c.inputFiles.files[idx].fileName = file
		c.inputFiles.files[idx].state = NotStarted
		c.inputFiles.files[idx].lastExecuteTime = time.Now()
		c.inputFiles.files[idx].lock = sync.Mutex{}
		c.inputFiles.files[idx].condition = sync.NewCond(&c.inputFiles.files[idx].lock)
		c.inputFiles.fileToIdx[file] = idx
	}

	fmt.Printf("input files %v\n", c.inputFiles.fileToIdx)

	for i := 0; i < nReduce; i++ {
		fileName := fmt.Sprintf("%d", i)
		c.intermediateFiles.files[i] = TargetFile{}
		c.intermediateFiles.files[i].fileName = fileName
		c.intermediateFiles.files[i].state = NotStarted
		c.intermediateFiles.files[i].lastExecuteTime = time.Now()
		c.intermediateFiles.files[i].lock = sync.Mutex{}
		c.intermediateFiles.files[i].condition = sync.NewCond(&c.intermediateFiles.files[i].lock)
		c.intermediateFiles.fileToIdx[fileName] = i
	}
	
    // go pprof routine
	go func() {
		runtime.SetBlockProfileRate(1)
		runtime.SetMutexProfileFraction(1)
		log.Println(http.ListenAndServe("localhost:6060", nil))
	}()
	
    // monitor routine
	go c.monitor()

	c.server()
	return &c
}
```

每个 worker 上线之后第一件事就是向 coordinator 申请一个全局唯一的 worker id

```go
// coordinator.go

// Your code here -- RPC handlers for the worker to call.
func (c *Coordinator) InitialRequest(request *RPCRequset, response *RPCResponse) error {
	c.lock.Lock()
	defer c.lock.Unlock()
	response.WorkerId = c.nextWorkerId
	c.nextWorkerId++
	response.NReduce = c.nReduce
	return nil
}
```

InitialRequest 的作用是返回 worker id 与 nReduce

随后 worker 通过 RPC 调用 RequestForTask 向 coordinator 获取 map/reduce task

```go
// coordinator.go

func (c *Coordinator) RequestForTask(request *RPCRequset, response *RPCResponse) error {
	fmt.Printf("worker %v request task, task type %v\n", request.WorkerId, request.TaskType)
	
    // worker 的每个 request 都暗含了对上一个 task 完成的响应, 所以先完成上一个 request
	c.TryToFinishFileState(request)
	
    // coordinator 先分配 map task, 再分配 reduce task
    // RequestForTargetFile 只有在当前类型的任务都已经完成了才会返回 nil
    // 如果当前类型的某个任务正在等待完成则会阻塞等待
	mapTaskFile := RequestForTargetFile(&c.inputFiles, request.WorkerId)
	if mapTaskFile != nil {
		response.TaskType = Map
		response.InputFile = mapTaskFile.fileName
		fmt.Printf("worker %v assign map task %v\n", request.WorkerId, mapTaskFile.fileName)
		return nil
	}

	reduceTask := RequestForTargetFile(&c.intermediateFiles, request.WorkerId)
	if reduceTask != nil {
		response.TaskType = Reduce
		response.InputFile = reduceTask.fileName
		fmt.Printf("worker %v assign reduce task %v\n", request.WorkerId, reduceTask.fileName)
		return nil
	}
	
    // 如果获取不到 map task, 也获取不到 reduce task, 那么任务一定都完成了, 返回 None 类型
	response.TaskType = None
	return nil
}
```

TryToFinishFileState 的作用是修改某个 task 的状态, 因为当前任务可能被分配给多个 worker 完成, 因此在修改状态之前需要获取锁对象

```go
// coordinator.go

func (c *Coordinator) TryToFinishFileState(request *RPCRequset) {
	// initial request, no file to finish
	if request.TaskType == None {
		return
	}
    
    // 根据任务类型找到对应的任务组
    var bundle *FileBundle
	if request.TaskType == Map {
		bundle = &c.inputFiles
	} else {
		bundle = &c.intermediateFiles
	}

	if idx, exist := bundle.fileToIdx[request.FinishedFile]; exist {
		file := &bundle.files[idx]
		fmt.Printf("worker %v try to finish %v\n", request.WorkerId, request.FinishedFile)
		file.lock.Lock()
		// 任务可能被分配给多个 worker, 为了保证只有一个 worker 可以最终完成当前任务, 对当前任务的状态, 超时时间进行限制
        // 只有当前任务尚未完成, 且当前 worker 未超时, 才能完成当前任务
		if file.state != Completed && time.Since(file.lastExecuteTime) < MAX_EXECUTE_TIME {
			fmt.Printf("worker %v finish %v\n", request.WorkerId, request.FinishedFile)
			file.state = Completed
            
			// 原子修改当前任务组的完成任务数量, 同步修改状态
			atomic.AddInt32(&bundle.doneCount, 1)
			if atomic.LoadInt32(&bundle.doneCount) == int32(len(bundle.files)) {
				bundle.finished = true
			}
			
            // 如果当前是 reduce 任务, 还需要修改文件名格式, 满足离线测试要求 ...
			if request.TaskType == Reduce {
				newPath := fmt.Sprintf("mr-out-%d", idx)
				err := os.Rename(request.IntermediateFile, newPath)
				if err != nil {
					log.Fatalf("cannot rename %v", err)
				}
			}
		} else {
            // 否则当前 worker 提交的任务是已经被完成/超时的了, 需要将其产生的输出文件删除掉
            // 如果是 map task 则会产生 nReduce 个 intermediate file, 如果是 reduce task 则会产生一个 output file
			fmt.Printf("worker %v timeout %v\n", request.WorkerId, request.FinishedFile)
			if request.TaskType == Map {
				// map task generate a bunch of intermediate file
				wildcard := fmt.Sprintf("%s*", request.IntermediateFile)
				files, err := filepath.Glob(wildcard)
				if err != nil {
					log.Fatalf("cannot find %v", err)
				}
				for _, f := range files {
					err = os.Remove(f)
					if err != nil {
						log.Fatalf("cannot remove %v", err)
					}
				}
			} else {
				// reduce task generate one intermediate file
				err := os.Remove(request.IntermediateFile)
				if err != nil {
					log.Fatalf("cannot remove %v", err)
				}
			}
		}
		// 不管当前任务是否被完成, 都需要唤醒所有阻塞在当前任务的协程 ...
		file.condition.Broadcast()
		file.lock.Unlock()
		fmt.Printf("worker %v return %v\n", request.WorkerId, request.FinishedFile)
	}
}
```

这里也能看出来当初 worker 进行复杂命名的原因了, 这里是为了避免 worker 直接生成 output file, 尽管每个 worker 在完成对应任务时都会生成 intermediate file, 但最终那些 intermediate file 是可用的取决于 coordinator, 而不是 worker (更本质上是出于 fault tolerance 的考虑)

RequestForTargetFile 的作用是从任务中选择一个任务返回给 worker, 整体上由两个循环构成, 第一个循环从所有尚未被分配出去 task /超时的 task 中选择一个返回, 第二个循环会遍历那些尚未超时的任务, 并阻塞当前协程, 直到超时 (monitor 负责超时检测)/任务完成 (TryToFinishFileState 会完成对应任务)

```go
// coordinator.go

func RequestForTargetFile(bundle *FileBundle, workerId int) *TargetFile {
	// 当前系列的任务都完成了, 直接返回即可
    if bundle.finished {
		return nil
	}
	
    // 通过取模的方式离散化当前枚举的起点
	for i, j := 0, workerId%len(bundle.files); i < len(bundle.files); i, j = i+1, j+1 {
		if j == len(bundle.files) {
			j = 0
		}
		file := &bundle.files[j]
		fmt.Printf("first: worker %v try to lock %v\n", workerId, file.fileName)
		// file state is completed, skip
		if file.state == Completed {
			continue
		}
		file.lock.Lock()
		fmt.Printf("first: worker %v locked %v\n", workerId, file.fileName)
        // 返回未完成/超时的任务
		if file.state == NotStarted || (file.state == InProgress && time.Since(file.lastExecuteTime) > MAX_EXECUTE_TIME) {
			file.state = InProgress
			file.lastExecuteTime = time.Now()
			file.lock.Unlock()
			fmt.Printf("first: worker %v get task %v\n", workerId, file.fileName)
			return file
		}
		file.lock.Unlock()
		fmt.Printf("first: worker %v unlock %v\n", workerId, file.fileName)
	}

	// 从剩下未超时的任务中阻塞等待任务的完成
	for i, j := 0, workerId%len(bundle.files); i <= len(bundle.files); i, j = i+1, j+1 {
		if j == len(bundle.files) {
			j = 0
		}
		file := &bundle.files[j]
		// complete work cannot be in progress again
		if file.state == Completed {
			continue
		}
		fmt.Printf("worker %v try to lock %v\n", workerId, file.fileName)
		file.lock.Lock()
		fmt.Printf("worker %v locked %v\n", workerId, file.fileName)
		// task has not started or timeout
		if file.state == InProgress {
			i = 0
			lastTime := file.lastExecuteTime
			sinceLast := time.Since(lastTime)
            // 如果当前任务超时, 则返回即可; 否则阻塞等待
			if sinceLast >= MAX_EXECUTE_TIME {
				// redo task immediately
				fmt.Printf("worker %v redo task %v\n", workerId, file.fileName)
				file.lastExecuteTime = time.Now()
				file.condition.Broadcast() // wake up the waiting worker
				file.lock.Unlock()
				fmt.Printf("worker %v release the lock %v\n", workerId, file.fileName)
				return file
			} else {
				// wait to redo task (wake up by other worker or monitor)
				fmt.Printf("worker %v wait to redo task %v\n", workerId, file.fileName)
				file.condition.Wait()
				// 当前协程被唤醒之后, 当前任务可能已经完成了/被分配给其他协程了
                // 所以需要检查一下当前任务的状态是否和阻塞之前一致, 在一致的情况下, 分配任务
				if file.state == InProgress && file.lastExecuteTime == lastTime {
					file.lastExecuteTime = time.Now()
					file.lock.Unlock()
					fmt.Printf("worker %v release the lock %v\n", workerId, file.fileName)
					return file
				}
			}
		}
		file.lock.Unlock()
		fmt.Printf("worker %v unlock %v\n", workerId, file.fileName)
	}

	return nil
}
```

# [Key/Value Server](https://pdos.csail.mit.edu/6.824/labs/lab-kvsrv.html)

简单来说这个 lab 需要实现一个 key-value server, 对于第一个 part 而言, 只要保证其是并发安全的即可

当前 lab 需要 key-value server 支持三种功能: 

*   Get: 返回当前 key 对应的 value, 如果当前 key 并不存在, 则返回空值
*   Put: 设置一对 key-value, 如果当前 key 已经存在了, 那么会替换当前 key 对应的 value; 这个方法不需要提供返回值, 返回空值即可
*   Append: 在当前 key 对应的 value 基础上 append 一个 value; 返回值为原始值, 如果之前没有保存当前 key, 则返回空值

Client 通过 Clerk 与 Server 交互, 每个 Clerk 会通过 RPC 的方式和 Server 交互

```go
// client.go

type Clerk struct {
	server *labrpc.ClientEnd
}

func MakeClerk(server *labrpc.ClientEnd) *Clerk {
	// You'll have to add code here.
	return &Clerk{server: server}
}

func (ck *Clerk) Get(key string) string {
	// You will have to modify this function.
	args := GetArgs{Key: key}
	var reply GetReply
    ck.server.Call("KVServer.Get", &args, &reply)
	return reply.Value
}

func (ck *Clerk) PutAppend(key string, value string, op string) string {
	// You will have to modify this function.
	args := PutAppendArgs{Key: key, Value: value}
	var reply PutAppendReply
    ck.server.Call("KVServer."+op, &args, &reply)
	return reply.Value
}
```

client.go 通过 PutAppend 函数概括了 Put/Append 两种行为, 对于 client 而言不需要考虑并发安全问题, 所以直接 RPC 调用即可

```go
// server.go

type KVServer struct {
	mu sync.Mutex

	// Your definitions here.
	kvs  map[string]string        // key-value store
}

func (kv *KVServer) Get(args *GetArgs, reply *GetReply) {
	// Your code here.
	kv.mu.Lock()
	defer kv.mu.Unlock()

	value, contains := kv.kvs[args.Key]
	// kv.mu.Unlock()
	if !contains {
		reply.Value = ""
	} else {
		reply.Value = value
	}
}

func (kv *KVServer) Put(args *PutAppendArgs, reply *PutAppendReply) {
	// Your code here.
	kv.mu.Lock()
	defer kv.mu.Unlock()

	kv.kvs[args.Key] = args.Value
	reply.Value = args.Value
}

func (kv *KVServer) Append(args *PutAppendArgs, reply *PutAppendReply) {
	// Your code here.
	kv.mu.Lock()
	defer kv.mu.Unlock()
    
	value, contains := kv.kvs[args.Key]
	if !contains {
		reply.Value = ""
		kv.kvs[args.Key] = args.Value
	} else {
		reply.Value = value
		kv.kvs[args.Key] = value + args.Value
	}
}

func StartKVServer() *KVServer {
	// You may need initialization code here.
	return &KVServer{mu: sync.Mutex{}, kvs: make(map[string]string)}
}
```

本质上一个 KV server 就是一个 map, 为了保证线程安全性, 加一个锁就好了 ...

本 lab 的难点在于第二个部分, 第二个部分需要考虑调用失败后, 重新调用的情况, 对于 client 的 Clerk 而言, RPC 调用失败后, 方法 Call 返回 false, 此时需要重新调用, 直到得到一个成功的结果 -> fault tolerance

一个 RPC 调用是双向的, Request/Response 都有可能出错, 如果是 Response 出错, 重复的 RPC 调用会导致 server 端 handler 执行多次的情况, 这个 lab 需要实现的 fault-tolerance 是 at-most-once

考虑一下三种操作, Get/Put/Append, 在网络不稳定的情况下均可能出现多次调用的情况, 这个 lab 后面一讲提到了 [linearizability](https://pdos.csail.mit.edu/6.824/notes/l-linearizability.txt) (说白了就是强一致性), 自然也是对当前 lab 的要求, 好在当前的 kv 程序 server 是单机部署的, 所以强一致性其实相对容易实现

*   多次 Get: 这个其实不需要 server 额外备份, 本身在 linearizability 的 history 中, linearizability point 可以位于 invocation 与 response 之间的任意一个时刻, client 多次重复得到的 response 可以是不同的 => 并发决定的
*   多次 Append: 这个是一定需要 server 额外备份的, client 的请求可以重复, 但是 server 的 value 不能因为网络的波动而增长多次
*   多次 Put: 比较反直觉的是, 这个也不需要 server 额外备份, 具体的原因可以参考多次 Get, 也是一样的, 可以认为 Put 的 linearizability point 更靠近 response 一点吧, 这样不管和 Put 并发的请求有多少, 都按最后一次成功的 Put 构成一个 order

综上, 其实只需要考虑实现对 Append 重复检测即可, 因此需要在 Append 的请求参数中额外添加当前请求 id, 当前请求的 client id; 而在 lab 中也提到了, 可以认为 client 不会并发调用 clerk, 即 client 在一个 clerk 调用返回之前是不会执行其他调用的, 因此 Append 请求 id 完全可以通过一个 bool 类型表示, 只需要分区前后两次 Append 即可

```go
// common.go

type MetaAppendInfo struct {
	// Your definitions here.
	ClerkId   int64
	RequestId bool // one client for a clerk, a bool request id is enough
}

// Put or Append
type PutAppendArgs struct {
	Key   string
	Value string
	// You'll have to add definitions here.
	// Field names must start with capital letters,
	// otherwise RPC will break.
	ClerkMeta MetaAppendInfo
}
```

修改 client 的 Get/Put/Append, 使用一个循环体包裹 RPC 调用, 直到调用成功

```go
// client.go

func (ck *Clerk) Get(key string) string {
	// You will have to modify this function.
	args := GetArgs{Key: key}
	var reply GetReply
	for {
		ok := ck.server.Call("KVServer.Get", &args, &reply)
		if ok {
			break
		}
	}
	return reply.Value
}

func (ck *Clerk) PutAppend(key string, value string, op string) string {
	// You will have to modify this function.
	args := PutAppendArgs{Key: key, Value: value, ClerkMeta: MetaAppendInfo{ClerkId: ck.id, RequestId: ck.op}}
	var reply PutAppendReply
	for {
		ok := ck.server.Call("KVServer."+op, &args, &reply)
		if ok {
			break
		}
	}
	// Only Append cannot be duplicated
	if op == "Append" {
		ck.op = !ck.op
	}
	return reply.Value
}
```

至于 server, 只需要修改 Append 即可, 使用一个 clerkid -> string 类型的 map 记录当前 clerk 上次 append 的返回值

```go
// server.go

type ResponseBuffer struct {
	RequestId bool
	Value     string
}

type KVServer struct {
	mu sync.Mutex

	// Your definitions here.
	kvs  map[string]string        // key-value store
	buff map[int64]ResponseBuffer // clerk id -> response buffer
}

func (kv *KVServer) Append(args *PutAppendArgs, reply *PutAppendReply) {
	// Your code here.
	kv.mu.Lock()
	defer kv.mu.Unlock()
	
    // 在 buff 中包含了当前 append id 时返回 buff 中的 string, 避免重复 append
	buff, contains := kv.buff[args.ClerkMeta.ClerkId]
	if contains && buff.RequestId == args.ClerkMeta.RequestId {
		reply.Value = buff.Value
		return
	}

	value, contains := kv.kvs[args.Key]
	if !contains {
		reply.Value = ""
		kv.kvs[args.Key] = args.Value
	} else {
		reply.Value = value
		kv.kvs[args.Key] = value + args.Value
	}
	
    // 更新 buff
	kv.buff[args.ClerkMeta.ClerkId] = ResponseBuffer{RequestId: args.ClerkMeta.RequestId, Value: reply.Value}
}
```





